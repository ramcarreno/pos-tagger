{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55e0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root)\n",
    "\n",
    "from validation import prob_of_error_propagation\n",
    "from src.scrapper import parse_conllu_file\n",
    "from src.tagger import HiddenMarkovModel, HiddenMarkovModelTagger\n",
    "from src.visualization import plot_viterbi_path_binary, plot_viterbi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7c20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = parse_conllu_file(filepath=\"../datasets/en_partut-ud-train.conllu\")\n",
    "test = parse_conllu_file(filepath=\"../datasets/en_partut-ud-test.conllu\")\n",
    "tagger = HiddenMarkovModel(corpus=train).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54daa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2710084033613445\n"
     ]
    }
   ],
   "source": [
    "corpus = test\n",
    "corpus_p = []\n",
    "for sentence in test:\n",
    "    s = reduce(lambda x, y: x + ' ' + y, map(lambda x: x[0], sentence))\n",
    "    _, s_p, _ = tagger.viterbi_best_path(s)\n",
    "    corpus_p.append(s_p)\n",
    "\n",
    "print(prob_of_error_propagation(corpus, corpus_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5eeef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all(corpus, tagger):\n",
    "    corpus_p = []\n",
    "    for sentence in corpus:\n",
    "        s = reduce(lambda x, y: x + ' ' + y, map(lambda x: x[0], sentence))\n",
    "        _, s_p, _ = tagger.viterbi_best_path(s)\n",
    "        corpus_p.append(s_p)\n",
    "\n",
    "    return corpus_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf441b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(corpus, corpus_p, tagset):\n",
    "    N = len(tagset)\n",
    "    cm = np.zeros((N, N))\n",
    "    for i in range(len(corpus)):\n",
    "        expected, prediction = corpus[i], corpus_p[i]\n",
    "        for token, token_p in zip(map(lambda x: x[1], expected), map(lambda x: x[1], prediction)):\n",
    "            cm[tagset.index(token), tagset.index(token_p)] += 1\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0421f081-05cc-48a3-bf6a-18e5ca8ba528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  1. 732.   1.   0.   0.   0.   0.   0.   0.   0.   9.   0.   0.   0.   0.   0.   9.   2.]\n",
      " [  1.   1. 215.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  17.   0.]\n",
      " [  1.  27.   0.  70.   2.  11.   0.   0.   0.   0.   8.   0.   0.   0.   0.   1.   8.   0.]\n",
      " [  1.   0.   0.   0. 434.   0.   0.   4.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  1.   2.   0.   0.   0. 485.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   1.  94.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  1.   1.   0.   0.  16.   0.   0.  86.   0.   0.   2.   0.   2.   0.   0.   1.   0.   0.]\n",
      " [  1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.]\n",
      " [  6.   0.   0.   0.   0.   0.   0.   0.   0. 333.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  1.  74.   0.   1.   0.   1.   0.   1.   0.   0. 131.   0.   0.   0.   0.   0.  13.   1.]\n",
      " [  0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.]\n",
      " [  0.  17.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.  42.   0.   0.   0.   1.   0.]\n",
      " [  0.   2.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   1.   0.]\n",
      " [  1.   0.   0.   0.   0.  33.   0.   0.   0.   1.   0.   0.   0.   0.  31.   0.   0.   0.]\n",
      " [  0.   0.   0.   2.   0.  13.   0.   7.   0.   0.   0.   0.   0.   0.   0.  29.   0.   0.]\n",
      " [  2.  95.  13.   0.   0.   1.   0.   0.   0.   0.   3.   0.   0.   0.   0.   0. 212.   0.]\n",
      " [  0.  45.   0.   0.   0.   0.   0.   0.   0.   0.   6.   0.   0.   0.   0.   0.   0.  39.]]\n"
     ]
    }
   ],
   "source": [
    "cm = get_confusion_matrix(corpus, corpus_p, tagger.tagset)\n",
    "cm_formatted = np.array2string(cm, precision=0, separator=' ', suppress_small=True, max_line_width=100)\n",
    "print(cm_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "933f3717-eaee-48a8-ade1-c45afb7590ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(confusion_matrix):\n",
    "    # get the matrix diagonal and sum all the correct predictions (true positives for each tag) \n",
    "    diagonal = np.diagonal(confusion_matrix)\n",
    "    total_correct = np.sum(diagonal)\n",
    "    \n",
    "    # sum the total number of predictions made\n",
    "    total_predictions = np.sum(confusion_matrix)\n",
    "    \n",
    "    # accuracy is defined as the ratio of correct predictions made\n",
    "    accuracy = total_correct / total_predictions\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f193d83-82c6-48da-b375-84d1b7f06f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision(confusion_matrix):\n",
    "    # number of tags\n",
    "    total_tags = len(confusion_matrix)\n",
    "\n",
    "    precisions, predictions = [], []\n",
    "\n",
    "    for i in range(total_tags):\n",
    "        # each column contains all the info we need\n",
    "        total_tag_correct = confusion_matrix[i, i]  # tp for that tag\n",
    "        total_tag_predictions = np.sum(confusion_matrix[:, i])\n",
    "\n",
    "        # precision is defined as the ratio of true predictions from all positive (applies to a certain tag)\n",
    "        if total_tag_predictions == 0:  # some tags may not appear in the test set at all \n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = total_tag_correct / total_tag_predictions\n",
    "        precisions.append(precision)\n",
    "        predictions.append(int(total_tag_predictions))\n",
    "\n",
    "    return precisions, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "643d8be7-9f32-41ff-bfce-7b16a0c41af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL ACCURACY\n",
      "--------------\n",
      "Model accuracy: 0.8599\n",
      "\n",
      "MODEL PRECISION\n",
      "---------------\n",
      "sym: 0.0000 over 17 predictions.\n",
      "noun: 0.7342 over 997 predictions.\n",
      "aux: 0.9389 over 229 predictions.\n",
      "adv: 0.9589 over 73 predictions.\n",
      "det: 0.9602 over 452 predictions.\n",
      "adp: 0.8899 over 545 predictions.\n",
      "cconj: 1.0000 over 94 predictions.\n",
      "pron: 0.8687 over 99 predictions.\n",
      "x: 0.0000 over 1 predictions.\n",
      "punct: 0.9970 over 334 predictions.\n",
      "adj: 0.8239 over 159 predictions.\n",
      "intj: 0.0000 over 0 predictions.\n",
      "num: 0.9545 over 44 predictions.\n",
      "_: 1.0000 over 1 predictions.\n",
      "part: 1.0000 over 31 predictions.\n",
      "sconj: 0.9355 over 31 predictions.\n",
      "verb: 0.8092 over 262 predictions.\n",
      "propn: 0.9070 over 43 predictions.\n",
      "\n",
      "Weighted average model precision: 0.8599\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL ACCURACY\")\n",
    "print(\"--------------\")\n",
    "acc = get_accuracy(cm)\n",
    "print(f\"Model accuracy: {acc:.4f}\")\n",
    "print()\n",
    "\n",
    "precs, preds = get_precision(cm)\n",
    "prec_data = zip(tagger.tagset, precs, preds)\n",
    "print(\"MODEL PRECISION\")\n",
    "print(\"---------------\")\n",
    "for tag in prec_data:  # iterate over all classes\n",
    "    print(f\"{tag[0]}: {tag[1]:.4f} over {int(tag[2])} predictions.\")\n",
    "\n",
    "# compute a weighted average\n",
    "total_precision = 0\n",
    "total_predictions = 0\n",
    "for precision, predictions in zip(precs, preds):\n",
    "    total_precision += precision * predictions\n",
    "    total_predictions += predictions\n",
    "weighted_precision = total_precision / total_predictions\n",
    "\n",
    "print(f\"\\nWeighted average model precision: {weighted_precision:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
