{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55e0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import sys\n",
    "root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(root)\n",
    "\n",
    "from validation import get_error_propagation_prob, get_accuracy, get_precision, get_recall, get_f1\n",
    "from src.scrapper import parse_conllu_file\n",
    "from src.tagger import HiddenMarkovModel, HiddenMarkovModelTagger\n",
    "from src.visualization import plot_viterbi_path_binary, plot_viterbi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7c20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data\n",
    "train = parse_conllu_file(filepath=\"../datasets/en_partut-ud-train.conllu\")\n",
    "test = parse_conllu_file(filepath=\"../datasets/en_partut-ud-test.conllu\")\n",
    "tagger = HiddenMarkovModel(corpus=train).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a641e21c-a7db-488d-ae67-acd594b1c131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2673684210526316\n"
     ]
    }
   ],
   "source": [
    "# Compute prediction data\n",
    "test_predictions = tagger.predict(corpus=test)\n",
    "print(get_error_propagation_prob(test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed21bebc-61df-42ba-911c-884431040595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[216   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0  17   0]\n",
      " [  0   1   0   0   0   0   0   0   2   0   0   0   0   0   0   0   1   0]\n",
      " [  1   0  31  33   1   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0 485   0   0   0   0   2   0   0   0   0   0   0   0   0   0]\n",
      " [  6   0   0   0 333   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  1   0   0   0   0  86   0   0   1  16   0   0   0   1   0   2   0   2]\n",
      " [  1   0   0  11   0   0  70   0  27   2   0   0   0   1   0   0   8   8]\n",
      " [  1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0]\n",
      " [  2   0   0   0   0   0   0   0 732   0   2   0   0   0   0   0   9   9]\n",
      " [  1   0   0   0   0   4   0   0   0 434   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  45   0  39   0   0   0   0   0   0   6]\n",
      " [  0   0   0   1   0   0   0   1   0   0   0  94   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0  13   0   7   2   0   0   0   0   0   0  29   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   1   0   0   0   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   1   0   0  17   0   0   0   0   0   0  42   1   0]\n",
      " [ 15   0   0   1   0   0   0   0  95   0   0   0   0   0   0   0 212   3]\n",
      " [  1   0   0   1   0   1   1   0  74   0   1   0   0   0   0   0  13 131]]\n"
     ]
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "tagset = tagger.tagset  # read corpus tagset\n",
    "cm = tagger.get_confusion_matrix(test, test_predictions)\n",
    "\n",
    "# Format matrix for pretty printing\n",
    "cm_formatted = np.array2string(cm, precision=0, separator=' ', suppress_small=True, max_line_width=100)\n",
    "print(cm_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643d8be7-9f32-41ff-bfce-7b16a0c41af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL ACCURACY\n",
      "--------------\n",
      "Model accuracy: 0.8602\n",
      "\n",
      "MODEL PRECISION\n",
      "---------------\n",
      "aux: 0.8780 over 246 predictions.\n",
      "_: 1.0000 over 1 predictions.\n",
      "part: 1.0000 over 31 predictions.\n",
      "adp: 0.8899 over 545 predictions.\n",
      "punct: 0.9970 over 334 predictions.\n",
      "pron: 0.8687 over 99 predictions.\n",
      "adv: 0.9589 over 73 predictions.\n",
      "x: 0.0000 over 1 predictions.\n",
      "noun: 0.7342 over 997 predictions.\n",
      "det: 0.9602 over 452 predictions.\n",
      "propn: 0.9070 over 43 predictions.\n",
      "cconj: 1.0000 over 94 predictions.\n",
      "sym: 0.0000 over 0 predictions.\n",
      "sconj: 0.9355 over 31 predictions.\n",
      "intj: 0.0000 over 0 predictions.\n",
      "num: 0.9545 over 44 predictions.\n",
      "verb: 0.8092 over 262 predictions.\n",
      "adj: 0.8239 over 159 predictions.\n",
      "\n",
      "Weighted average model micro precision: 0.8713\n",
      "Average model macro precision: 0.7621\n",
      "\n",
      "MODEL RECALL\n",
      "------------\n",
      "aux: 0.9231 over 234 predictions.\n",
      "_: 0.2500 over 4 predictions.\n",
      "part: 0.4697 over 66 predictions.\n",
      "adp: 0.9939 over 488 predictions.\n",
      "punct: 0.9823 over 339 predictions.\n",
      "pron: 0.7890 over 109 predictions.\n",
      "adv: 0.5469 over 128 predictions.\n",
      "x: 0.0000 over 2 predictions.\n",
      "noun: 0.9708 over 754 predictions.\n",
      "det: 0.9886 over 439 predictions.\n",
      "propn: 0.4333 over 90 predictions.\n",
      "cconj: 0.9792 over 96 predictions.\n",
      "sym: 0.0000 over 0 predictions.\n",
      "sconj: 0.5686 over 51 predictions.\n",
      "intj: 0.0000 over 2 predictions.\n",
      "num: 0.6885 over 61 predictions.\n",
      "verb: 0.6503 over 326 predictions.\n",
      "adj: 0.5874 over 223 predictions.\n",
      "\n",
      "Weighted average model micro recall: 0.8989\n",
      "Average model macro recall: 0.6012\n",
      "\n",
      "MODEL F1\n",
      "------------\n",
      "Model micro F1: 0.8849\n",
      "Model macro F1: 0.6721\n"
     ]
    }
   ],
   "source": [
    "# Accuracy calculations\n",
    "print(\"MODEL ACCURACY\")\n",
    "print(\"--------------\")\n",
    "\n",
    "acc = get_accuracy(cm)\n",
    "print(f\"Model accuracy: {acc:.4f}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Precision calculations\n",
    "print(\"MODEL PRECISION\")\n",
    "print(\"---------------\")\n",
    "\n",
    "precs, p_preds, p_micro, p_macro = get_precision(cm)\n",
    "prec_data = zip(tagset, precs, p_preds)\n",
    "for tag in prec_data:  # iterate over all classes\n",
    "    print(f\"{tag[0]}: {tag[1]:.4f} over {int(tag[2])} predictions.\")\n",
    "\n",
    "print(f\"\\nWeighted average model micro precision: {p_micro:.4f}\")\n",
    "print(f\"Average model macro precision: {p_macro:.4f}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Recall calculations\n",
    "print(\"MODEL RECALL\")\n",
    "print(\"------------\")\n",
    "\n",
    "recalls, r_preds, r_micro, r_macro = get_recall(cm)\n",
    "recall_data = zip(tagset, recalls, r_preds)\n",
    "for tag in recall_data:  # iterate over all classes\n",
    "    print(f\"{tag[0]}: {tag[1]:.4f} over {int(tag[2])} predictions.\")\n",
    "\n",
    "print(f\"\\nWeighted average model micro recall: {r_micro:.4f}\")\n",
    "print(f\"Average model macro recall: {r_macro:.4f}\")\n",
    "print()\n",
    "\n",
    "# F1 calculations\n",
    "print(\"MODEL F1\")\n",
    "print(\"------------\")\n",
    "micro_f1 = get_f1(p_micro, r_micro)\n",
    "macro_f1 = get_f1(p_macro, r_macro)\n",
    "print(f\"Model micro F1: {micro_f1:.4f}\")\n",
    "print(f\"Model macro F1: {macro_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
